{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-27 12:32:55.134112: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-27 12:32:55.163722: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-05-27 12:32:55.164207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-27 12:32:55.704041: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.14) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "[nltk_data] Downloading package wordnet to /home/jj/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw to /home/jj/nltk_data...\n",
      "[nltk_data]   Package omw is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/jj/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "100%|██████████| 60001/60001 [02:57<00:00, 338.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# from generator import *\n",
    "from search import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "term:\n",
      "ass\n",
      "words:\n",
      "['ass']\n",
      "documento:\n",
      "134\n",
      "resultado:\n",
      "{134: 0.03201547669582878}\n",
      "documento:\n",
      "181\n",
      "resultado:\n",
      "{134: 0.03201547669582878, 181: 0.02662784700886242}\n",
      "documento:\n",
      "241\n",
      "resultado:\n",
      "{134: 0.03201547669582878, 181: 0.02662784700886242, 241: 0.17507847587180503}\n",
      "documento:\n",
      "246\n",
      "resultado:\n",
      "{134: 0.03201547669582878, 181: 0.02662784700886242, 241: 0.17507847587180503, 246: 0.16831003234889008}\n",
      "result:\n",
      "[(0.17507847587180503, 241)]\n",
      "0.45904173106646057\n",
      "prob:\n",
      "[[-inf   0.]]\n",
      "m:\n",
      "1.0\n",
      "prob:\n",
      "0.0\n",
      "th:\n",
      "0.0\n",
      "threshold:\n",
      "0.2\n",
      "resultado abaixo do threshold especificado :(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jj/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:1402: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.log(self.predict_proba(X))\n"
     ]
    }
   ],
   "source": [
    "#coloquei prints para ver o que estava acontecendo\n",
    "#TODO - achar o erro\n",
    "#     - concertar o erro\n",
    "#     - remover os prints\n",
    "\n",
    "\n",
    "command = \"!search ass th=0.2\"\n",
    "result = tfidf_search(command)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45904173106646057\n",
      "prob:\n",
      "[[-6.49761461e+00 -1.50816647e-03]]\n",
      "m:\n",
      "0.9984929702383343\n",
      "prob:\n",
      "0.001510442755384922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001510442755384922"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "command = \"!search ass th=0.1\"\n",
    "content_filter(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1038/60001 [00:03<03:27, 283.62it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m     index[w] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m     11\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(tfidf\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m---> 12\u001b[0m         \u001b[39mif\u001b[39;00m tfidf[j, vectorizer\u001b[39m.\u001b[39;49mvocabulary_[w]] \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     13\u001b[0m             index[w][j] \u001b[39m=\u001b[39m tfidf[j, vectorizer\u001b[39m.\u001b[39mvocabulary_[w]]\n\u001b[1;32m     16\u001b[0m result \u001b[39m=\u001b[39m query(\u001b[39m\"\u001b[39m\u001b[39mwiki\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m, index)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/_index.py:52\u001b[0m, in \u001b[0;36mIndexMixin.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(row, INT_TYPES):\n\u001b[1;32m     51\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(col, INT_TYPES):\n\u001b[0;32m---> 52\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_intXint(row, col)\n\u001b[1;32m     53\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(col, \u001b[39mslice\u001b[39m):\n\u001b[1;32m     54\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_on_1d_array_slice()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/scipy/sparse/_compressed.py:657\u001b[0m, in \u001b[0;36m_cs_matrix._get_intXint\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    655\u001b[0m M, N \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    656\u001b[0m major, minor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_swap((row, col))\n\u001b[0;32m--> 657\u001b[0m indptr, indices, data \u001b[39m=\u001b[39m get_csr_submatrix(\n\u001b[1;32m    658\u001b[0m     M, N, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindptr, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata,\n\u001b[1;32m    659\u001b[0m     major, major \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m, minor, minor \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[1;32m    660\u001b[0m \u001b[39mreturn\u001b[39;00m data\u001b[39m.\u001b[39msum(dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "DATASET = 'crawler_data.csv'\n",
    "df = pd.read_csv(DATASET)\n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(df['body'])\n",
    "index = dict()\n",
    "\n",
    "for w in tqdm(vectorizer.vocabulary_.keys()):\n",
    "    index[w] = dict()\n",
    "    for j in range(tfidf.shape[0]):\n",
    "        if tfidf[j, vectorizer.vocabulary_[w]] > 0:\n",
    "            index[w][j] = tfidf[j, vectorizer.vocabulary_[w]]\n",
    "\n",
    "\n",
    "result = query(\"wiki\", 1, index)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = r\"!search (.+)(?:\\sth=(\\d+(\\.\\d+)?))?\"\n",
    "match = re.match(pattern, command)\n",
    "\n",
    "if match:\n",
    "    term = match.group(1)\n",
    "    threshold = match.group(2)\n",
    "        \n",
    "    if threshold is not None:\n",
    "        threshold = float(threshold)\n",
    "\n",
    "# else:\n",
    "#     match = re.match(r\"!search (.+)\", command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def content_filter(content):\n",
    "\n",
    "    bad_words = 'datasets/bad_words.csv'\n",
    "    good_words = 'datasets/words_pos.csv'\n",
    "\n",
    "    bad_words = pd.read_csv(bad_words)\n",
    "    good_words = pd.read_csv(good_words)\n",
    "\n",
    "\n",
    "    good_words = good_words.drop(columns=['pos_tag'])\n",
    "    good_words['IsBad'] = 0\n",
    "    bad_words['IsBad'] = 1\n",
    "\n",
    "    good_words_sample = good_words.sample(1618, random_state=42)\n",
    "\n",
    "    words = pd.concat([good_words_sample, bad_words])\n",
    "\n",
    "    X = words[\"word\"]\n",
    "    y = words[\"IsBad\"]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.2, random_state=42)\n",
    "\n",
    "    classificador = Pipeline([\n",
    "                        ('meu_vetorizador', CountVectorizer(stop_words='english')),\n",
    "                        ('meu_classificador', LogisticRegression(penalty=None, solver='saga', max_iter=10000))\n",
    "                        ])\n",
    "    \n",
    "    classificador.fit(X_train,y_train)\n",
    "    y_pred = classificador.predict(X_test)\n",
    "    acc = accuracy_score(y_pred,y_test)\n",
    "    print(acc)\n",
    "\n",
    "    prob = classificador.predict_log_proba([content])\n",
    "    probas = classificador.predict_proba(X_train)\n",
    "\n",
    "    m = np.max(probas)\n",
    "    prob = 2 * (m - prob) / (2 * m) - 1\n",
    "    return prob[0][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.45904173106646057\n"
     ]
    }
   ],
   "source": [
    "th = content_filter(\"ass\")\n",
    "\n",
    "if th > 0.5:\n",
    "    print(\"yup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bad_words = 'datasets/bad_words.csv'\n",
    "good_words = 'datasets/words_pos.csv'\n",
    "\n",
    "bad_words = pd.read_csv(bad_words)\n",
    "good_words = pd.read_csv(good_words)\n",
    "\n",
    "\n",
    "good_words = good_words.drop(columns=['pos_tag'])\n",
    "good_words['IsBad'] = 0\n",
    "bad_words['IsBad'] = 1\n",
    "\n",
    "good_words_sample = good_words.sample(1618, random_state=42)\n",
    "\n",
    "words = pd.concat([good_words_sample, bad_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>IsBad</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>219193</th>\n",
       "      <td>overemployment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119423</th>\n",
       "      <td>frostflower</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251834</th>\n",
       "      <td>progeneration</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319490</th>\n",
       "      <td>terracer</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258313</th>\n",
       "      <td>quemeful</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1612</th>\n",
       "      <td>cocky</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1613</th>\n",
       "      <td>transsexual</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1614</th>\n",
       "      <td>unfuckable</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1615</th>\n",
       "      <td>bestiality</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1616</th>\n",
       "      <td>cocklicker</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3235 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  word  IsBad\n",
       "219193  overemployment      0\n",
       "119423     frostflower      0\n",
       "251834   progeneration      0\n",
       "319490        terracer      0\n",
       "258313        quemeful      0\n",
       "...                ...    ...\n",
       "1612             cocky      1\n",
       "1613       transsexual      1\n",
       "1614        unfuckable      1\n",
       "1615        bestiality      1\n",
       "1616        cocklicker      1\n",
       "\n",
       "[3235 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pato\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "pattern = r\"!search (.+)(?:\\sth=(?:\\d+(\\.\\d+))?)?\"\n",
    "pattern2 = r\"(.+)(?:\\s)\"\n",
    "pattern3 = r\".*?th=(\\d+(\\.\\d+)?)\"\n",
    "\n",
    "\n",
    "command = \"!search pato th=0.5\"\n",
    "groups= re.match(pattern, command)\n",
    "\n",
    "term = re.match(pattern2 , groups.group(1))\n",
    "term = term.group(1)\n",
    "\n",
    "threshold  = re.match(pattern3, groups.group(1))\n",
    "threshold = threshold.group(1)\n",
    "\n",
    "print(term)\n",
    "print(threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
