{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ", because it's so amazing.\n",
      "\n",
      "I mean: here we are all the way back home, still looking at the ocean and I almost feel like I'm about to die.\n",
      "\n",
      "What do you do after you die, when you don't know how?\n",
      "\n",
      "When I'm not doing things, I go back to my old life and make\n"
     ]
    }
   ],
   "source": [
    "def extract_after_first_string(text, first_string):\n",
    "    pattern = re.escape(first_string) + r\"(.+)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)  # Add re.DOTALL flag to match newline characters\n",
    "    if match:\n",
    "        extracted_text = match.group(1)\n",
    "        return extracted_text\n",
    "    return None\n",
    "\n",
    "def gpt2_generate(command):\n",
    "    match = re.match(r\"!gpt2_generate:(.+)\", command)\n",
    "    content = match.group(1)\n",
    "\n",
    "    generator = pipeline('text-generation', model='gpt2')\n",
    "    set_seed(42)\n",
    "    gpt_output = generator(content, max_length=80, num_return_sequences=1)\n",
    "\n",
    "    generated_text = gpt_output[0]['generated_text']  # Access the generated text from the output\n",
    "    return generated_text\n",
    "    # return extract_after_first_string(generated_text, content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ion accident. The whole series is all on the Internet at http://puu.sh/z6sKM. In another post about the incident, she states that she found a text message from one of her friends who is also an accordion player in a room. She says that the text wasn't about the accordion and that he\n"
     ]
    }
   ],
   "source": [
    "print(gpt2_generate(\"!gpt2_generate: make me a poem about a 1997 honda accord\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
